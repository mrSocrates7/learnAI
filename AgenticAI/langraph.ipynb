{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Agents in LangGraph\n",
        "- LangGraph is introduced by LangChain which is an open source framework for building LLM applications.\n",
        "- LangGraph allows to create highly controllable agents\n",
        "- Research papers used\n",
        "  - https://arxiv.org/pdf/2210.03629.pdf\n",
        "  - https://arxiv.org/pdf/2303.17651.pdf\n",
        "  - https://arxiv.org/pdf/2401.08500.pdf\n",
        "  "
      ],
      "metadata": {
        "id": "4S2U5Z56Q-T4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Agents from Scratch without frameworks\n",
        "- Agent we are going to build is based on the ReAct paper"
      ],
      "metadata": {
        "id": "g0o7WYzT12p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing ReAct Agent from scratch\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import httpx\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello world\"}]\n",
        ")\n",
        "\n",
        "chat_completion.choices[0].message.content\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, system=\"\"):\n",
        "        self.system = system\n",
        "        self.messages = []\n",
        "        if self.system:\n",
        "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
        "\n",
        "    def __call__(self, message):\n",
        "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
        "        result = self.execute()\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
        "        return result\n",
        "\n",
        "    def execute(self):\n",
        "        completion = client.chat.completions.create(\n",
        "                        model=\"gpt-4o\",\n",
        "                        temperature=0,\n",
        "                        messages=self.messages)\n",
        "        return completion.choices[0].message.content\n",
        "\n",
        "\n",
        "prompt = \"\"\"\n",
        "You run in a loop of Thought, Action, PAUSE, Observation.\n",
        "At the end of the loop you output an Answer\n",
        "Use Thought to describe your thoughts about the question you have been asked.\n",
        "Use Action to run one of the actions available to you - then return PAUSE.\n",
        "Observation will be the result of running those actions.\n",
        "\n",
        "Your available actions are:\n",
        "\n",
        "calculate:\n",
        "e.g. calculate: 4 * 7 / 3\n",
        "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
        "\n",
        "average_dog_weight:\n",
        "e.g. average_dog_weight: Collie\n",
        "returns average weight of a dog when given the breed\n",
        "\n",
        "Example session:\n",
        "\n",
        "Question: How much does a Bulldog weigh?\n",
        "Thought: I should look the dogs weight using average_dog_weight\n",
        "Action: average_dog_weight: Bulldog\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: A Bulldog weights 51 lbs\n",
        "\n",
        "You then output:\n",
        "\n",
        "Answer: A bulldog weights 51 lbs\n",
        "\"\"\".strip()\n",
        "\n",
        "def calculate(what):\n",
        "    return eval(what)\n",
        "\n",
        "def average_dog_weight(name):\n",
        "    if name in \"Scottish Terrier\":\n",
        "        return(\"Scottish Terriers average 20 lbs\")\n",
        "    elif name in \"Border Collie\":\n",
        "        return(\"a Border Collies average weight is 37 lbs\")\n",
        "    elif name in \"Toy Poodle\":\n",
        "        return(\"a toy poodles average weight is 7 lbs\")\n",
        "    else:\n",
        "        return(\"An average dog weights 50 lbs\")\n",
        "\n",
        "known_actions = {\n",
        "    \"calculate\": calculate,\n",
        "    \"average_dog_weight\": average_dog_weight\n",
        "}\n",
        "\n",
        "abot = Agent(prompt)\n",
        "\n",
        "result = abot(\"How much does a toy poodle weigh?\")\n",
        "print(result)\n",
        "\n",
        "result = average_dog_weight(\"Toy Poodle\")\n",
        "\n",
        "next_prompt = \"Observation: {}\".format(result)\n",
        "abot(next_prompt)\n",
        "abot.messages\n",
        "abot = Agent(prompt)\n",
        "question = \"\"\"I have 2 dogs, a border collie and a scottish terrier. \\\n",
        "What is their combined weight\"\"\"\n",
        "abot(question)\n",
        "\n",
        "next_prompt = \"Observation: {}\".format(average_dog_weight(\"Border Collie\"))\n",
        "print(next_prompt)\n",
        "abot(next_prompt)\n",
        "\n",
        "next_prompt = \"Observation: {}\".format(average_dog_weight(\"Scottish Terrier\"))\n",
        "print(next_prompt)\n",
        "abot(next_prompt)\n",
        "\n",
        "next_prompt = \"Observation: {}\".format(eval(\"37 + 20\"))\n",
        "print(next_prompt)\n",
        "\n",
        "\n",
        "abot(next_prompt)\n",
        "\n",
        "# Adding loop to automate\n",
        "\n",
        "action_re = re.compile('^Action: (\\w+): (.*)$')   # python regular expression to selection action\n",
        "\n",
        "def query(question, max_turns=5):\n",
        "    i = 0\n",
        "    bot = Agent(prompt)\n",
        "    next_prompt = question\n",
        "    while i < max_turns:\n",
        "        i += 1\n",
        "        result = bot(next_prompt)\n",
        "        print(result)\n",
        "        actions = [\n",
        "            action_re.match(a)\n",
        "            for a in result.split('\\n')\n",
        "            if action_re.match(a)\n",
        "        ]\n",
        "        if actions:\n",
        "            # There is an action to run\n",
        "            action, action_input = actions[0].groups()\n",
        "            if action not in known_actions:\n",
        "                raise Exception(\"Unknown action: {}: {}\".format(action, action_input))\n",
        "            print(\" -- running {} {}\".format(action, action_input))\n",
        "            observation = known_actions[action](action_input)\n",
        "            print(\"Observation:\", observation)\n",
        "            next_prompt = \"Observation: {}\".format(observation)\n",
        "        else:\n",
        "            return\n",
        "\n",
        "question = \"\"\"I have 2 dogs, a border collie and a scottish terrier. \\\n",
        "What is their combined weight\"\"\"\n",
        "query(question)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oryjS-_vAjJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangGraph Components\n",
        "- LangChain: Prompts\n",
        "  - reusable prompts\n",
        "- LangChain: Tools\n",
        "  - TavilySearchTool\n",
        "- Nodes : Agents or functions\n",
        "  -\n",
        "- Edges: Connect nodes\n",
        "- Conditional Edges: decisions\n",
        "- State: StateGraph\n",
        "  - Agent state is accessible to all parts of the graph\n",
        "  - It is local to the graph\n",
        "  - Can be stored in a persistence layer\n",
        "\n",
        "#### Features of LangGraph\n",
        "- Cyclic Graphs\n",
        "- Persistence\n",
        "- Human-in-the loop\n",
        "\n"
      ],
      "metadata": {
        "id": "Cy4dAOQiEcLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, HumanMess\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.tool.tavily_search import TavilySearchResults\n",
        "\n",
        "# Creating tool to be used later\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "print(type(tool))\n",
        "print(tool.name)\n",
        "\n",
        "# Creating State object, it is a dictionary with specific elements\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list[AnyMessage], operator.add]\n",
        "\n",
        "# Agent with 3 functions: Call openAI, Check if an action is present, take action\n",
        "class Agent:\n",
        "  def __init__(self, model, tools, system=\"\"):\n",
        "    self.system=system\n",
        "    graph = StateGraph(AgentState)\n",
        "    graph.add_node(\"llm\", self.call_openai)\n",
        "    graph.add_node(\"action\", self.takeaction)\n",
        "    graph.add_conditional_edges(\n",
        "        \"llm\",\n",
        "        self.exists_action,\n",
        "        {True: \"action\", False: END}\n",
        "    )\n",
        "    graph.add_edge(\"action\", \"llm\")\n",
        "    graph.set_entry_point(\"llm\")\n",
        "    self.graph = graph.compile()\n",
        "    self.tools = {t.name: t for t in tools}\n",
        "    self.model = model.bind_tools(tools)\n",
        "\n",
        "  def call_openai(self, state: AgentState):\n",
        "    messages = state['messages']\n",
        "    if self.system:\n",
        "      messages = [SystemMessage(content=self.system)] + messages\n",
        "    message = self.model.invoke(messages)\n",
        "    return {'messages': [message]}\n",
        "\n",
        "  def take_action(self, state: AgentState):\n",
        "    tool_calls = state['messages'][-1].tool_calls\n",
        "    results = []\n",
        "    for t in tool_calls:\n",
        "      print(f\"Calling: {t}\")\n",
        "      result = self.tools[t['name']].invoke(t['args'])\n",
        "      results.append(ToolMessage(too_call_id=t['id'], name=t['name']))\n",
        "    print(\"Back to the model!\")\n",
        "    return {'messages': results}\n",
        "\n",
        "  def exists_action(self, state: AgentState):\n",
        "    result = state['messages'][-1]\n",
        "    return len(result.tool_calls) > 0\n",
        "\n",
        "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
        "You are allowed to make multiple calls (either together or in sequence). \\\n",
        "Only look up information when you are sure of what you want. \\\n",
        "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
        "\"\"\"\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\")  #reduce inference cost\n",
        "abot = Agent(model, [tool], system=prompt)\n",
        "\n",
        "# Visualizing graph\n",
        "from Ipython.display import Image\n",
        "\n",
        "Image(abot.graph.get_graph().draw_png())\n",
        "\n",
        "messages = [HumanMessage(content=\"What is the weather in sf?\")]\n",
        "result = abot.graph.invoke({\"messages\": messages})"
      ],
      "metadata": {
        "id": "wHNCAtmcLJI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Persistence and Streaming\n",
        "- Persistence\n",
        "  - Checkpointer\n",
        "- Streaming\n",
        "  - Streaming messages\n",
        "  - Streaming tokens"
      ],
      "metadata": {
        "id": "OqJalhSwGos2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, HumanMess\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.tool.tavily_search import TavilySearchResults\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "\n",
        "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
        "\n",
        "# Creating tool to be used later\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "print(type(tool))\n",
        "print(tool.name)\n",
        "\n",
        "# Creating State object, it is a dictionary with specific elements\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list[AnyMessage], operator.add]\n",
        "\n",
        "# Agent with 3 functions: Call openAI, Check if an action is present, take action\n",
        "class Agent:\n",
        "  def __init__(self, model, tools, checkpointer, system=\"\"):\n",
        "    self.system=system\n",
        "    graph = StateGraph(AgentState)\n",
        "    graph.add_node(\"llm\", self.call_openai)\n",
        "    graph.add_node(\"action\", self.takeaction)\n",
        "    graph.add_conditional_edges(\n",
        "        \"llm\",\n",
        "        self.exists_action,\n",
        "        {True: \"action\", False: END}\n",
        "    )\n",
        "    graph.add_edge(\"action\", \"llm\")\n",
        "    graph.set_entry_point(\"llm\")\n",
        "    self.graph = graph.compile(checkpointer = checkpointer)\n",
        "    self.tools = {t.name: t for t in tools}\n",
        "    self.model = model.bind_tools(tools)\n",
        "\n",
        "  def call_openai(self, state: AgentState):\n",
        "    messages = state['messages']\n",
        "    if self.system:\n",
        "      messages = [SystemMessage(content=self.system)] + messages\n",
        "    message = self.model.invoke(messages)\n",
        "    return {'messages': [message]}\n",
        "\n",
        "  def take_action(self, state: AgentState):\n",
        "    tool_calls = state['messages'][-1].tool_calls\n",
        "    results = []\n",
        "    for t in tool_calls:\n",
        "      print(f\"Calling: {t}\")\n",
        "      result = self.tools[t['name']].invoke(t['args'])\n",
        "      results.append(ToolMessage(too_call_id=t['id'], name=t['name']))\n",
        "    print(\"Back to the model!\")\n",
        "    return {'messages': results}\n",
        "\n",
        "  def exists_action(self, state: AgentState):\n",
        "    result = state['messages'][-1]\n",
        "    return len(result.tool_calls) > 0\n",
        "\n",
        "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
        "You are allowed to make multiple calls (either together or in sequence). \\\n",
        "Only look up information when you are sure of what you want. \\\n",
        "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
        "\"\"\"\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\")  #reduce inference cost\n",
        "abot = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
        "\n",
        "# Visualizing graph\n",
        "from Ipython.display import Image\n",
        "\n",
        "Image(abot.graph.get_graph().draw_png())\n",
        "\n",
        "messages = [HumanMessage(content=\"What is the weather in sf?\")]\n",
        "\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
        "  for v in event.values():\n",
        "    print(v['messages'])\n",
        "\n",
        "# Streaming tokens\n",
        "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
        "\n",
        "memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n",
        "abot = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
        "\n",
        "messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
        "thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
        "async for event in abot.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
        "    kind = event[\"event\"]\n",
        "    if kind == \"on_chat_model_stream\":\n",
        "        content = event[\"data\"][\"chunk\"].content\n",
        "        if content:\n",
        "            # Empty content in the context of OpenAI means\n",
        "            # that the model is asking for a tool to be invoked.\n",
        "            # So we only print non-empty content\n",
        "            print(content, end=\"|\")"
      ],
      "metadata": {
        "id": "73IFHRqzIL6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Human in the loop\n",
        "-"
      ],
      "metadata": {
        "id": "aKxkBbTcLBV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, HumanMess\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.tool.tavily_search import TavilySearchResults\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "\n",
        "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
        "\n",
        "from uuid import uuid4\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "\"\"\"\n",
        "In previous examples we've annotated the `messages` state key\n",
        "with the default `operator.add` or `+` reducer, which always\n",
        "appends new messages to the end of the existing messages array.\n",
        "\n",
        "Now, to support replacing existing messages, we annotate the\n",
        "`messages` key with a customer reducer function, which replaces\n",
        "messages with the same `id`, and appends them otherwise.\n",
        "\"\"\"\n",
        "def reduce_messages(left: list[AnyMessage], right: list[AnyMessage]) -> list[AnyMessage]:\n",
        "    # assign ids to messages that don't have them\n",
        "    for message in right:\n",
        "        if not message.id:\n",
        "            message.id = str(uuid4())\n",
        "    # merge the new messages with the existing messages\n",
        "    merged = left.copy()\n",
        "    for message in right:\n",
        "        for i, existing in enumerate(merged):\n",
        "            # replace any existing messages with the same id\n",
        "            if existing.id == message.id:\n",
        "                merged[i] = message\n",
        "                break\n",
        "        else:\n",
        "            # append any new messages to the end\n",
        "            merged.append(message)\n",
        "    return merged\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], reduce_messages]\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, model, tools, system=\"\", checkpointer=None):\n",
        "        self.system = system\n",
        "        graph = StateGraph(AgentState)\n",
        "        graph.add_node(\"llm\", self.call_openai)\n",
        "        graph.add_node(\"action\", self.take_action)\n",
        "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
        "        graph.add_edge(\"action\", \"llm\")\n",
        "        graph.set_entry_point(\"llm\")\n",
        "        self.graph = graph.compile(\n",
        "            checkpointer=checkpointer,\n",
        "            interrupt_before=[\"action\"]\n",
        "        )\n",
        "        self.tools = {t.name: t for t in tools}\n",
        "        self.model = model.bind_tools(tools)\n",
        "\n",
        "    def call_openai(self, state: AgentState):\n",
        "        messages = state['messages']\n",
        "        if self.system:\n",
        "            messages = [SystemMessage(content=self.system)] + messages\n",
        "        message = self.model.invoke(messages)\n",
        "        return {'messages': [message]}\n",
        "\n",
        "    def exists_action(self, state: AgentState):\n",
        "        print(state)\n",
        "        result = state['messages'][-1]\n",
        "        return len(result.tool_calls) > 0\n",
        "\n",
        "    def take_action(self, state: AgentState):\n",
        "        tool_calls = state['messages'][-1].tool_calls\n",
        "        results = []\n",
        "        for t in tool_calls:\n",
        "            print(f\"Calling: {t}\")\n",
        "            result = self.tools[t['name']].invoke(t['args'])\n",
        "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
        "        print(\"Back to the model!\")\n",
        "        return {'messages': results}\n",
        "\n",
        "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
        "You are allowed to make multiple calls (either together or in sequence). \\\n",
        "Only look up information when you are sure of what you want. \\\n",
        "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
        "\"\"\"\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "abot = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
        "\n",
        "messages = [HumanMessage(content=\"Whats the weather in SF?\")]\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
        "    for v in event.values():\n",
        "        print(v)\n",
        "\n",
        "\n",
        "abot.graph.get_state(thread)\n",
        "abot.graph.get_state(thread).next\n",
        "\n",
        "for event in abot.graph.stream(None, thread):\n",
        "    for v in event.values():\n",
        "        print(v)\n",
        "\n",
        "abot.graph.get_state(thread)\n",
        "abot.graph.get_state(thread).next\n",
        "\n",
        "messages = [HumanMessage(\"Whats the weather in LA?\")]\n",
        "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
        "    for v in event.values():\n",
        "        print(v)\n",
        "while abot.graph.get_state(thread).next:\n",
        "    print(\"\\n\", abot.graph.get_state(thread),\"\\n\")\n",
        "    _input = input(\"proceed?\")\n",
        "    if _input != \"y\":\n",
        "        print(\"aborting\")\n",
        "        break\n",
        "    for event in abot.graph.stream(None, thread):\n",
        "        for v in event.values():\n",
        "            print(v)\n",
        "\n",
        "messages = [HumanMessage(\"Whats the weather in LA?\")]\n",
        "thread = {\"configurable\": {\"thread_id\": \"3\"}}\n",
        "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
        "    for v in event.values():\n",
        "        print(v)\n",
        "\n",
        "abot.graph.get_state(thread)\n",
        "current_values = abot.graph.get_state(thread)\n",
        "current_values.values['messages'][-1]\n",
        "current_values.values['messages'][-1].tool_calls\n",
        "\n",
        "_id = current_values.values['messages'][-1].tool_calls[0]['id']\n",
        "current_values.values['messages'][-1].tool_calls = [\n",
        "    {'name': 'tavily_search_results_json',\n",
        "  'args': {'query': 'current weather in Louisiana'},\n",
        "  'id': _id}\n",
        "]\n",
        "\n",
        "abot.graph.update_state(thread, current_values.values)\n",
        "abot.graph.get_state(thread)\n",
        "\n",
        "for event in abot.graph.stream(None, thread):\n",
        "    for v in event.values():\n",
        "        print(v)\n",
        "states = []\n",
        "for state in abot.graph.get_state_history(thread):\n",
        "    print(state)\n",
        "    print('--')\n",
        "    states.append(state)\n",
        "\n",
        "to_replay = states[-3]\n",
        "to_replay\n",
        "\n",
        "for event in abot.graph.stream(None, to_replay.config):\n",
        "    for k, v in event.items():\n",
        "        print(v)\n",
        "\n",
        "to_replay\n",
        "\n",
        "_id = to_replay.values['messages'][-1].tool_calls[0]['id']\n",
        "to_replay.values['messages'][-1].tool_calls = [{'name': 'tavily_search_results_json',\n",
        "  'args': {'query': 'current weather in LA, accuweather'},\n",
        "  'id': _id}]\n",
        "\n",
        "branch_state = abot.graph.update_state(to_replay.config, to_replay.values)\n",
        "\n",
        "for event in abot.graph.stream(None, branch_state):\n",
        "    for k, v in event.items():\n",
        "        if k != \"__end__\":\n",
        "            print(v)\n",
        "\n",
        "to_replay\n",
        "_id = to_replay.values['messages'][-1].tool_calls[0]['id']\n",
        "\n",
        "state_update = {\"messages\": [ToolMessage(\n",
        "    tool_call_id=_id,\n",
        "    name=\"tavily_search_results_json\",\n",
        "    content=\"54 degree celcius\",\n",
        ")]}\n",
        "\n",
        "branch_and_add = abot.graph.update_state(\n",
        "    to_replay.config,\n",
        "    state_update,\n",
        "    as_node=\"action\")\n",
        "\n",
        "for event in abot.graph.stream(None, branch_and_add):\n",
        "    for k, v in event.items():\n",
        "        print(v)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jrj4nCTWNbX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Essay Writer example\n",
        "-"
      ],
      "metadata": {
        "id": "tuAKtdocSERz"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "langraph",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}