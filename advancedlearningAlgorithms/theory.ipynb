{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf5KrEb6vrkR"
      },
      "source": [
        "# Advanced Learning Algorithms\n",
        "- Neural networks\n",
        "  - inference (prediction)\n",
        "  - training\n",
        "- Practical advice for building ML systems\n",
        "- Decision Trees\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks\n",
        "- Attempt to mimic how the human brain functions\n",
        "- Re-branded in 2005 as Deep Learning\n",
        "- Applied in - Speech Recognition, Computer vision, Natural Language Processing etc\n",
        "- The neural networks depends on the amount of data available. The more the data, the better will be the neural network performance\n",
        "- Small, Medium, Large neural networks\n",
        "- Example : Predicting if a t-shirt is going to be top-selling or not based on a single factor price\n",
        "  - x --> Input\n",
        "  - a = Activation = f(x) = $\\frac{1}{1+e^{-(wx+b)}}$ --> Output\n",
        "  - this logistic regression model can be a single neuron in the overall neural network with multiple other models working collectively\n",
        "- Complex example : Predicting if a t-shirt is going to be top-selling or not with price, shipping cost, marketing, material as inputs\n",
        "  - Affordability --> Price, Shipping cost --> one neuron\n",
        "  - Awareness --> Marketing --> one neuron\n",
        "  - Perceived Quality --> Material --> one neuron\n",
        "  - It is not a strict rule that price, shipping cost are inputs to only first neuron. All data can be input to all neurons in the next layer and then the neuron will decide which feature or value to use\n",
        "  - $Input Layer (\\vec X) â‡’ hidden layer â‡’ (\\vec a) â‡’ Output layer â‡’ Finaloutput(a)$\n"
      ],
      "metadata": {
        "id": "YVzA9ggWtxBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Model\n",
        "- Fundamental building block of nerual network is layer of neural network layer or layer of neurons\n",
        "- layer 0 --> layer 1 --> layer 2 --> layer 3 --> layer 4\n",
        "- $\\vec a^{[1]}$ --> vector a represents the ouput of a layer and [1] represents the layer number\n",
        "- $a_j^{[l]}=g(\\vec w_j^{[l]}.\\vec a_j^{[l-1]}+b_j^{[l]})$\n",
        "  - w,b parameters of layer l, unit/neuron j\n",
        "  - a is the input from previous layer l-1\n",
        "  - Sigmoid activation function\n",
        "\n"
      ],
      "metadata": {
        "id": "cxyMv1Iw6E8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference : Making predictions (Forward propagation)\n",
        "- Let's suppose we have one input layer, 3 hidden layers and one output layer\n",
        "- $\\vec X$ is the input layer : layer 0\n",
        "- Layer 1 : Let's suppose there are 10 neurons in this layer taking $\\vec X$ as input. Then the equations for z in this layer will be\n",
        "  - $\\vec W_1^{[1]}.\\vec X + b_1^{[1]}$ ........ $\\vec W_{10}^{[1]}.\\vec X + b_{10}^{[1]}$\n",
        "  - outputs $\\vec a^{[1]}$\n",
        "- Layer 2\n",
        "  - number of neurons 4\n",
        "  - input $\\vec a^{[1]}$\n",
        "  - equations\n",
        "    - $\\vec W_1^{[2]}.\\vec a^{[1]} + b_1^{[2]}$ ........ $\\vec W_4^{[2]}.\\vec a^{[1]} + b_4^{[2]}$\n",
        "  - outputs $\\vec a^{[2]}$\n",
        "- Layer 3\n",
        "  - number of neurons 2\n",
        "  - input $\\vec a^{[2]}$\n",
        "  - equations\n",
        "    - $\\vec W_1^{[3]}.\\vec a^{[2]} + b_1^{[3]}$ and $\\vec W_2^{[3]}.\\vec a^{[2]} + b_2^{[3]}$\n",
        "  - outputs $\\vec a^{[3]}$\n",
        "- Layer 4 - output layer\n",
        "  - number of neurons 1\n",
        "  - input $\\vec a^{[3]}$\n",
        "  - equations\n",
        "    - $\\vec W^{[4]}.\\vec a^{[3]} + b^{[4]}$\n",
        "  - outputs $a^{[4]}$"
      ],
      "metadata": {
        "id": "odakk56EiFlB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a neural network in TensorFlow\n",
        "- layer1 = Dense(units=3, activation=\"sigmoid\")\n",
        "- layer2 = Dense(units=1, activation=\"sigmoid\")\n",
        "- model = Sequential([layer1, layer2])\n",
        "- x = np.array([[200.0, 17.0],[120.0, 5.0],[425.0, 20.0],[212.0, 18.0]])\n",
        "- y = np.array([1,0,0,1])\n",
        "- model.compile()\n",
        "- model.fit(x,y)\n",
        "- y_new = model.predict(x_new)\n"
      ],
      "metadata": {
        "id": "qF7vKnfXSujq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coffee roasting example using TensorFlow\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def load_coffee_data():\n",
        "    \"\"\" Creates a coffee roasting data set.\n",
        "        roasting duration: 12-15 minutes is best\n",
        "        temperature range: 175-260C is best\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(2)\n",
        "    X = rng.random(400).reshape(-1,2)\n",
        "    X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best\n",
        "    X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best\n",
        "    Y = np.zeros(len(X))\n",
        "\n",
        "    i=0\n",
        "    for t,d in X:\n",
        "        y = -3/(260-175)*t + 21\n",
        "        if (t > 175 and t < 260 and d > 12 and d < 15 and d<=y ):\n",
        "            Y[i] = 1\n",
        "        else:\n",
        "            Y[i] = 0\n",
        "        i += 1\n",
        "\n",
        "    return (X, Y.reshape(-1,1))\n",
        "\n",
        "X,Y = load_coffee_data()\n",
        "print(X.shape, Y.shape)\n",
        "\n",
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(2,)),\n",
        "        Dense(3, activation='sigmoid', name = 'layer1'),\n",
        "        Dense(1, activation='sigmoid', name = 'layer2')\n",
        "     ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "L1_num_params = 2 * 3 + 3   # W1 parameters  + b1 parameters\n",
        "L2_num_params = 3 * 1 + 1   # W2 parameters  + b2 parameters\n",
        "print(\"L1 params = \", L1_num_params, \", L2 params = \", L2_num_params  )\n",
        "\n",
        "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
        "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
        "print(f\"W1{W1.shape}:\\n\", W1, f\"\\nb1{b1.shape}:\", b1)\n",
        "print(f\"W2{W2.shape}:\\n\", W2, f\"\\nb2{b2.shape}:\", b2)\n",
        "\n",
        "model.compile(\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    Xt,Yt,\n",
        "    epochs=10,\n",
        ")\n",
        "\n",
        "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
        "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
        "print(\"W1:\\n\", W1, \"\\nb1:\", b1)\n",
        "print(\"W2:\\n\", W2, \"\\nb2:\", b2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cjFYRKY4Vdyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network implementation in Python\n",
        "#### Forward propagation in a single layer\n",
        "- x = np.array([200, 17])\n",
        "- $a_1^{[1]}=g(\\vec W_1^{[1]}.\\vec X+b_1^{[1]})$\n",
        "- $a_2^{[1]}=g(\\vec W_2^{[1]}.\\vec X+b_2^{[1]})$\n",
        "- $a_3^{[1]}=g(\\vec W_3^{[1]}.\\vec X+b_3^{[1]})$\n",
        "- $W_1^{[2]}$ will be represented as W2_1 in the code\n",
        "- $a_1^{[2]}=g(\\vec W_1^{[2]}.\\vec a^{[1]}+b_1^{[2]})$"
      ],
      "metadata": {
        "id": "wdBNZgnGYsMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([200, 17])\n",
        "w1_1 = np.array([1,2])\n",
        "b1_1 = np.array([-1])\n",
        "z1_1 = np.dot(w1_1, x) + b1_1\n",
        "a1_1 = sigmoid(z1_1)\n",
        "print(a1_1)\n",
        "\n",
        "w1_2 = np.array([-3,4])\n",
        "b1_2 = np.array([1])\n",
        "z1_2 = np.dot(w1_2, x) + b1_2\n",
        "a1_2 = sigmoid(z1_2)\n",
        "print(a1_2)\n",
        "\n",
        "w1_3 = np.array([5,-6])\n",
        "b1_3 = np.array([2])\n",
        "z1_3 = np.dot(w1_3, x) + b1_3\n",
        "a1_3 = sigmoid(z1_3)\n",
        "print(a1_3)\n",
        "\n",
        "a1 = np.array([a1_1, a1_2, a1_3])\n",
        "\n",
        "w2_1 = np.array([-7, 8, 9])\n",
        "b2_1 = np.array([3])\n",
        "z2_1 = np.dot(w2_1, a1) + b2_1\n",
        "a2_1 = sigmoid(z2_1)\n",
        "print(a2_1)"
      ],
      "metadata": {
        "id": "ix_5rxskT4Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network implementation in Python\n",
        "#### General implementation of forward propagation\n"
      ],
      "metadata": {
        "id": "NTWeRc72XAJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dense(a_in,W,b):\n",
        "  units = W.shape[1]\n",
        "  a_out = np.zeros(units)\n",
        "  for j in range(units):\n",
        "    w = W[:,j]\n",
        "    z = np.dot(w,a_in) + b[j]\n",
        "    a_out[j] = g(z)\n",
        "  return a_out\n",
        "\n",
        "def g(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "def sequential(x):\n",
        "  a1 = dense(x,W1,b1)\n",
        "  a2 = dense(a1,W2,b2)\n",
        "  a3 = dense(a2,W3,b3)\n",
        "  a4 = dense(a3,W4,b4)\n",
        "  f_x = a4\n",
        "  return f_x\n",
        "\n",
        "W = np.array([[1, -3, 5],[2, 4, -6]])\n",
        "b = np.array([-1, 1, 2])\n",
        "a_in = np.array([-2, 4])"
      ],
      "metadata": {
        "id": "yLWP6VNoXMdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speculations on Artificial General Intelligence AGI\n",
        "- AI\n",
        "  - ANI (artificial narrow intelligence) : smart speaker, self driving car, web search, AI in farming and factories etc\n",
        "  - AGI (artificial general intelligence) : Do anything a human can do\n",
        "- Experiments\n",
        "  - Roe et al - 1992\n",
        "  -"
      ],
      "metadata": {
        "id": "SjmSbS2W8b0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization : To implement neural networks efficiently\n",
        "- Instead of using normal numpy array, we have to use them as matrices to take advantage of matrix multiplication"
      ],
      "metadata": {
        "id": "KppO7Y06-hWZ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Theory",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}