{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf5KrEb6vrkR"
      },
      "source": [
        "# Advanced Learning Algorithms\n",
        "- Neural networks\n",
        "  - inference (prediction)\n",
        "  - training\n",
        "- Practical advice for building ML systems\n",
        "- Decision Trees\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks\n",
        "- Attempt to mimic how the human brain functions\n",
        "- Re-branded in 2005 as Deep Learning\n",
        "- Applied in - Speech Recognition, Computer vision, Natural Language Processing etc\n",
        "- The neural networks depends on the amount of data available. The more the data, the better will be the neural network performance\n",
        "- Small, Medium, Large neural networks\n",
        "- Example : Predicting if a t-shirt is going to be top-selling or not based on a single factor price\n",
        "  - x --> Input\n",
        "  - a = Activation = f(x) = $\\frac{1}{1+e^{-(wx+b)}}$ --> Output\n",
        "  - this logistic regression model can be a single neuron in the overall neural network with multiple other models working collectively\n",
        "- Complex example : Predicting if a t-shirt is going to be top-selling or not with price, shipping cost, marketing, material as inputs\n",
        "  - Affordability --> Price, Shipping cost --> one neuron\n",
        "  - Awareness --> Marketing --> one neuron\n",
        "  - Perceived Quality --> Material --> one neuron\n",
        "  - It is not a strict rule that price, shipping cost are inputs to only first neuron. All data can be input to all neurons in the next layer and then the neuron will decide which feature or value to use\n",
        "  - $Input Layer (\\vec X) ⇒ hidden layer ⇒ (\\vec a) ⇒ Output layer ⇒ Finaloutput(a)$\n"
      ],
      "metadata": {
        "id": "YVzA9ggWtxBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Model\n",
        "- Fundamental building block of nerual network is layer of neural network or layer of neurons\n",
        "- layer 0 --> layer 1 --> layer 2 --> layer 3 --> layer 4\n",
        "- $\\vec a^{[l]}$ --> vector a represents the ouput of a layer and [l] represents the layer number\n",
        "- $a_j^{[l]}=g(\\vec w_j^{[l]}.\\vec a_j^{[l-1]}+b_j^{[l]})$\n",
        "  - w,b parameters of layer l, unit/neuron j\n",
        "  - a is the input from previous layer l-1\n",
        "  - Sigmoid activation function\n",
        "\n"
      ],
      "metadata": {
        "id": "cxyMv1Iw6E8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference : Making predictions (Forward propagation)\n",
        "- Let's suppose we have one input layer, 3 hidden layers and one output layer\n",
        "- $\\vec X$ is the input layer : layer 0\n",
        "- Layer 1 : Let's suppose there are 10 neurons in this layer taking $\\vec X$ as input. Then the equations for z in this layer will be\n",
        "  - $\\vec W_1^{[1]}.\\vec X + b_1^{[1]}$\n",
        "  - ........\n",
        "  - $\\vec W_{10}^{[1]}.\\vec X + b_{10}^{[1]}$\n",
        "  - outputs $\\vec a^{[1]}$\n",
        "- Layer 2\n",
        "  - number of neurons 4\n",
        "  - input $\\vec a^{[1]}$\n",
        "  - equations\n",
        "    - $\\vec W_1^{[2]}.\\vec a^{[1]} + b_1^{[2]}$ ........ $\\vec W_4^{[2]}.\\vec a^{[1]} + b_4^{[2]}$\n",
        "  - outputs $\\vec a^{[2]}$\n",
        "- Layer 3\n",
        "  - number of neurons 2\n",
        "  - input $\\vec a^{[2]}$\n",
        "  - equations\n",
        "    - $\\vec W_1^{[3]}.\\vec a^{[2]} + b_1^{[3]}$ and $\\vec W_2^{[3]}.\\vec a^{[2]} + b_2^{[3]}$\n",
        "  - outputs $\\vec a^{[3]}$\n",
        "- Layer 4 - output layer\n",
        "  - number of neurons 1\n",
        "  - input $\\vec a^{[3]}$\n",
        "  - equations\n",
        "    - $\\vec W^{[4]}.\\vec a^{[3]} + b^{[4]}$\n",
        "  - outputs $a^{[4]}$"
      ],
      "metadata": {
        "id": "odakk56EiFlB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a neural network in TensorFlow\n",
        "- layer1 = Dense(units=3, activation=\"sigmoid\")\n",
        "- layer2 = Dense(units=1, activation=\"sigmoid\")\n",
        "- model = Sequential([layer1, layer2])\n",
        "- x = np.array([[200.0, 17.0],[120.0, 5.0],[425.0, 20.0],[212.0, 18.0]])\n",
        "- y = np.array([1,0,0,1])\n",
        "- model.compile()\n",
        "- model.fit(x,y)\n",
        "- y_new = model.predict(x_new)\n"
      ],
      "metadata": {
        "id": "qF7vKnfXSujq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coffee roasting example using TensorFlow\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def load_coffee_data():\n",
        "    \"\"\" Creates a coffee roasting data set.\n",
        "        roasting duration: 12-15 minutes is best\n",
        "        temperature range: 175-260C is best\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(2)\n",
        "    X = rng.random(400).reshape(-1,2)\n",
        "    X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best\n",
        "    X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best\n",
        "    Y = np.zeros(len(X))\n",
        "\n",
        "    i=0\n",
        "    for t,d in X:\n",
        "        y = -3/(260-175)*t + 21\n",
        "        if (t > 175 and t < 260 and d > 12 and d < 15 and d<=y ):\n",
        "            Y[i] = 1\n",
        "        else:\n",
        "            Y[i] = 0\n",
        "        i += 1\n",
        "\n",
        "    return (X, Y.reshape(-1,1))\n",
        "\n",
        "X,Y = load_coffee_data()\n",
        "print(X.shape, Y.shape)\n",
        "\n",
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(2,)),\n",
        "        Dense(3, activation='sigmoid', name = 'layer1'),\n",
        "        Dense(1, activation='sigmoid', name = 'layer2')\n",
        "     ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "L1_num_params = 2 * 3 + 3   # W1 parameters  + b1 parameters\n",
        "L2_num_params = 3 * 1 + 1   # W2 parameters  + b2 parameters\n",
        "print(\"L1 params = \", L1_num_params, \", L2 params = \", L2_num_params  )\n",
        "\n",
        "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
        "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
        "print(f\"W1{W1.shape}:\\n\", W1, f\"\\nb1{b1.shape}:\", b1)\n",
        "print(f\"W2{W2.shape}:\\n\", W2, f\"\\nb2{b2.shape}:\", b2)\n",
        "\n",
        "model.compile(\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    Xt,Yt,\n",
        "    epochs=10,\n",
        ")\n",
        "\n",
        "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
        "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
        "print(\"W1:\\n\", W1, \"\\nb1:\", b1)\n",
        "print(\"W2:\\n\", W2, \"\\nb2:\", b2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cjFYRKY4Vdyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network implementation in Python\n",
        "#### Forward propagation in a single layer\n",
        "- x = np.array([200, 17])\n",
        "- $a_1^{[1]}=g(\\vec W_1^{[1]}.\\vec X+b_1^{[1]})$\n",
        "- $a_2^{[1]}=g(\\vec W_2^{[1]}.\\vec X+b_2^{[1]})$\n",
        "- $a_3^{[1]}=g(\\vec W_3^{[1]}.\\vec X+b_3^{[1]})$\n",
        "- $W_1^{[2]}$ will be represented as W2_1 in the code\n",
        "- $a_1^{[2]}=g(\\vec W_1^{[2]}.\\vec a^{[1]}+b_1^{[2]})$"
      ],
      "metadata": {
        "id": "wdBNZgnGYsMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([200, 17])\n",
        "w1_1 = np.array([1,2])\n",
        "b1_1 = np.array([-1])\n",
        "z1_1 = np.dot(w1_1, x) + b1_1\n",
        "a1_1 = sigmoid(z1_1)\n",
        "print(a1_1)\n",
        "\n",
        "w1_2 = np.array([-3,4])\n",
        "b1_2 = np.array([1])\n",
        "z1_2 = np.dot(w1_2, x) + b1_2\n",
        "a1_2 = sigmoid(z1_2)\n",
        "print(a1_2)\n",
        "\n",
        "w1_3 = np.array([5,-6])\n",
        "b1_3 = np.array([2])\n",
        "z1_3 = np.dot(w1_3, x) + b1_3\n",
        "a1_3 = sigmoid(z1_3)\n",
        "print(a1_3)\n",
        "\n",
        "a1 = np.array([a1_1, a1_2, a1_3])\n",
        "\n",
        "w2_1 = np.array([-7, 8, 9])\n",
        "b2_1 = np.array([3])\n",
        "z2_1 = np.dot(w2_1, a1) + b2_1\n",
        "a2_1 = sigmoid(z2_1)\n",
        "print(a2_1)"
      ],
      "metadata": {
        "id": "ix_5rxskT4Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network implementation in Python\n",
        "#### General implementation of forward propagation\n"
      ],
      "metadata": {
        "id": "NTWeRc72XAJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dense(a_in,W,b):\n",
        "  units = W.shape[1]\n",
        "  a_out = np.zeros(units)\n",
        "  for j in range(units):\n",
        "    w = W[:,j]\n",
        "    z = np.dot(w,a_in) + b[j]\n",
        "    a_out[j] = g(z)\n",
        "  return a_out\n",
        "\n",
        "def g(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "def sequential(x):\n",
        "  a1 = dense(x,W1,b1)\n",
        "  a2 = dense(a1,W2,b2)\n",
        "  a3 = dense(a2,W3,b3)\n",
        "  a4 = dense(a3,W4,b4)\n",
        "  f_x = a4\n",
        "  return f_x\n",
        "\n",
        "W = np.array([[1, -3, 5],[2, 4, -6]])\n",
        "b = np.array([-1, 1, 2])\n",
        "a_in = np.array([-2, 4])"
      ],
      "metadata": {
        "id": "yLWP6VNoXMdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speculations on Artificial General Intelligence AGI\n",
        "- AI\n",
        "  - ANI (artificial narrow intelligence) : smart speaker, self driving car, web search, AI in farming and factories etc\n",
        "  - AGI (artificial general intelligence) : Do anything a human can do\n",
        "- Experiments\n",
        "  - Roe et al - 1992\n",
        "  -"
      ],
      "metadata": {
        "id": "SjmSbS2W8b0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization : To implement neural networks efficiently\n",
        "- Instead of using normal numpy array, we have to use them as matrices to take advantage of matrix multiplication\n",
        "\n",
        "#### Matrix Multiplication\n",
        "- Matrix is a 2D array of numbers\n",
        "- $z=\\vec a.\\vec w$ = $a^T\\vec W$\n",
        "- Vector matrix multiplication\n"
      ],
      "metadata": {
        "id": "KppO7Y06-hWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix multiplication code\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[1,-1,0.1],[2,-2,0.2]])\n",
        "AT = np.array([[1,2],[-1,-2],[0.1,0.2]])\n",
        "AT = A.T # another way to get transpose of matrix\n",
        "W = np.array([[3,5,7,9],[4,6,8,0]])\n",
        "z = np.matmul(AT,W) # alternate way to call matmul is z = AT @ W\n"
      ],
      "metadata": {
        "id": "vhi5-B2RfEDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Training\n",
        "\n"
      ],
      "metadata": {
        "id": "GqOiMHUMyAfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Training in TensorFlow\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(units=25, activation='sigmoid'),\n",
        "    Dense(units=15, activation='sigmoid'),\n",
        "    Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss=BinaryCrossentropy())\n",
        "model.fit(X,Y,epochs=100) # epochs is the number of steps the algorithms can run, similar to number of steps in gradient descent\n",
        "\n"
      ],
      "metadata": {
        "id": "LPZgUJVxzrPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What's going on in the background\n",
        "- In general the model training steps are\n",
        "  - Define model. For example in Logistic regression specify how to compute output given input x and parameters w,b\n",
        "  - $f_{\\vec w,b}(\\vec X) = g(z) = \\frac {1}{1+e^{-z}} = \\frac {1}{1+e^{-(\\vec W.\\vec X + b)}} $\n",
        "  - Specify loss and cost functions\n",
        "  - $L(f_{\\vec w,b}(\\vec x^i), y^i)$ = $-y^ilog(f_{\\vec w,b}(\\vec x^i))-(1-y^i)log(1 - f_{\\vec w,b}(\\vec x^i))$\n",
        "  - Define cost function\n",
        "  - $J_{\\vec w, b}$ = $\\frac{1}{m}\\sum_{i=1}^m[L(f_{\\vec w,b}(\\vec x^i), y^i)]$\n",
        "  - Train on data to minimize cost function (like using gradient descent)\n",
        "    - repeat simultaneously,\n",
        "    - $w_j = w_j - α \\frac {\\partial}{\\partial w_j}J(\\vec w,b)$\n",
        "    - $b= b - α \\frac {\\partial}{\\partial b}J(\\vec w,b)$\n",
        "- How is it done in Neural network using Tensorflow\n",
        "  - Define model\n",
        "    - model = Sequential([Dense(...),Dense(...),Dense(...)])\n",
        "  - Compile and specify loss function\n",
        "    - model.compile(loss=BinaryCrossentropy())\n",
        "  - Minimize the costfunction\n",
        "    - model.fit(X,Y,epochs=100)"
      ],
      "metadata": {
        "id": "pYi6nv1o1rZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation Functions\n",
        "- For binary : Sigmoid : $g(z) = \\frac {1}{1+e^{-z}}$\n",
        "- ReLU (Rectified Linear Unit) : $g(z) = max(0,z)$\n",
        "- Linear activation function : $g(z) = z = \\vec w.\\vec x + b$\n",
        "\n",
        "### Choosing activation functions\n",
        "- Different activation functions can be used for different neurons on a layer\n",
        "- For output layer :\n",
        "  - for binary classification : Sigmoid\n",
        "  - Regression (output is both positive and negative values) : Linear activation Function\n",
        "  - Regression (output is only positive values) : ReLU activation function\n",
        "- For hidden layers :\n",
        "  - ReLU us most common choice\n",
        "  - Sigmoid/Linear Regression are very rarely used"
      ],
      "metadata": {
        "id": "66KO9UvyWVva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiclass Classification\n",
        "- Classification problems where output labels are more than 2\n",
        "- Softmax regression algorithm is a generalization of logistic regression from binary classification to multiclass classification\n",
        "\n",
        "#### Softmax regression\n",
        "- Suppose for a problem being solved has 4 possible outputs (y=1,2,3,4)\n",
        "- then we will be calculating $z_1, z_2, z_3, z_4$ each corresponding to output types as below\n",
        "  - $z_1 = \\vec w_1.\\vec x+b_1$\n",
        "  - $z_2 = \\vec w_2.\\vec x+b_2$\n",
        "  - $z_3 = \\vec w_3.\\vec x+b_3$\n",
        "  - $z_4 = \\vec w_4.\\vec x+b_4$\n",
        "- then the outputs $a_1, a_2, a_3, a_4$ will be calculated as below\n",
        "  - $a_1 = \\frac {e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}}$\n",
        "  - $a_2 = \\frac {e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}}$\n",
        "  - $a_3 = \\frac {e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}}$\n",
        "  - $a_4 = \\frac {e^{z_4}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}}$\n",
        "\n",
        "#### Generalizing the Softmax Regression\n",
        "- Number of possible outputs = N\n",
        "- calculate $z_j = \\vec w_j.\\vec x+b_j$ ; j = 1,2,......,N\n",
        "- calculate $a_j = \\frac {e^{z_j}}{\\sum_{k=1}^Ne^{z_k}}=P(y=j|\\vec x)$\n",
        "\n",
        "#### Cost function for Softmax Regression\n",
        "- Equations used\n",
        "  - $a_1 = \\frac {e^{z_1}}{e^{z_1}+e^{z_2}+....+e^{z_N}}$ .....\n",
        "  - $a_N = \\frac {e^{z_N}}{e^{z_1}+e^{z_2}+....+e^{z_N}}$\n",
        "- loss($a_1,...,a_N,y$) = $-log\\ a_1\\ (if\\ y=1)\\ ; -log\\ a_2\\ (if y=2)\\ ; .... ; -log\\ a_N\\ (if\\ y=N)$\n",
        "- that is loss = -log $a_j$ if y=j\n"
      ],
      "metadata": {
        "id": "N_89SssM76qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network with Softmax output\n",
        "- Let's suppose we are building a neural network to recognize a given digit with 2 hidden layers and one output layer. Then the output layer will have 10 nodes each to indicate the probability of the number being the 10 numerical digits.\n",
        "- The hidden layer will use relu and the output layer will use Softmax\n",
        "- Specify the model\n",
        "  - import tensorflow as tf\n",
        "  - from tensorflow.keras import Sequential\n",
        "  - from tensorflow.keras.layers import Dense\n",
        "  - model = Sequential([Dense(units=25, activation='relu'),Dense(units=15, activation='relu'),Dense(units=10, activation='softmax')])\n",
        "- Specify loss and cost\n",
        "  - from tensorflow.keras.losses import SparseCategoricalCrossEntropy\n",
        "  - model.compile(loss=SparseCategoricalCrossEntropy())\n",
        "- Train on data to minimize cost\n",
        "  - model.fit(X,Y,epochs=100)\n",
        "\n",
        "#### Improved implementation of softmax\n",
        "- Specify the model\n",
        "  - import tensorflow as tf\n",
        "  - from tensorflow.keras import Sequential\n",
        "  - from tensorflow.keras.layers import Dense\n",
        "  - model = Sequential([Dense(units=25, activation='relu'),Dense(units=15, activation='relu'),Dense(units=10, activation='softmax')])\n",
        "- Specify loss and cost\n",
        "  - from tensorflow.keras.losses import SparseCategoricalCrossEntropy\n",
        "  - model.compile(loss=SparseCategoricalCrossEntropy(from_logits=True))\n",
        "- Train on data to minimize cost\n",
        "  - model.fit(X,Y,epochs=100)\n"
      ],
      "metadata": {
        "id": "D719qZFOAeJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Back Propagation\n",
        "- Calculating Derivatives"
      ],
      "metadata": {
        "id": "HepjxxeIDMuY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Theory",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}