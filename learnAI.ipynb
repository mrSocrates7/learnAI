{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning\n",
        "#### Field of study that gives machines the ability to learn without being explicitly programmed"
      ],
      "metadata": {
        "id": "gvFB7aa5TvFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Algorithms:\n",
        "- Supervised Learning Algorithms\n",
        "- Unsupervised Learning Algorithms\n",
        "- Recommender Systems\n",
        "- Reinforcement Learning Algorithms"
      ],
      "metadata": {
        "id": "6ms_8ygXTiPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised Learning Algorithms:\n",
        "- Algorithms that learn from input, output labels and then predict the output for a given new input\n",
        "- Example:\n",
        "  - Email(Input) -> spam or not (Output) : Spam filtering\n",
        "  - Image & Radar Info (Input) -> position of other cars (Output) : Self driving cars\n",
        "- Types of Supervised Learning\n",
        "  - Regression : *Predicting a number from infinitely many possible ouputs*\n",
        "  - Classification : *Predicting a category*\n",
        "\n",
        "## Unsupervised Learning Algorithms:\n",
        "- Algorithms that find something interesting in unlabeled data\n",
        "- Types of Unsupervised Learning Algorithms\n",
        "  - Clustering\n",
        "  - Association Rules\n",
        "  - DImensionality reduction : Compress data using fewer numbers"
      ],
      "metadata": {
        "id": "rwLcezsYT8qX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Learning Algorithms:\n",
        "## Regression Model : Predicting numbers\n",
        "### Linear Regression with one Variable:\n",
        "- Suppose we have existing data of x, y where for different values of x, the value of y changes\n",
        "- Example : based on the size of the flat area, rent of the house varies. And we have historical data for 10 different sizes, the rent values. Now for a given new input of size, the model has to predict the relevant rent.\n",
        "- Terminology\n",
        "  - Data used to train model (historical data) -> training set\n",
        "  - Input variable or feature -> x\n",
        "  - Original Output variable or target -> y\n",
        "  - Number of training examples -> m\n",
        "  - Single training example will be (x,y)\n",
        "- Now we have to choose a linear function (since this is linear regression) which can be used to predict the y values for a given x\n",
        "- Let the linear Function (Our Model) *$f_{(w,b)}$(x) = wx + b*\n",
        "  - We can choose the model as per needs and the number of variables.\n",
        "  - For example if we have multiple (lets say 3) variables but we want to use linear function then the model becomes\n",
        "    - *$f_{(w,b)}(x) = w_1x_1 +w_2x_2 + w_3x_3 + b$*\n",
        "  - If we want to use polynomial regression wtih one variable then the function can look like\n",
        "    - *$f_{(w,b)}(x) = w_1x^1 +w_2x^2 + w_3x^3 + b$*\n",
        "- We have to find best values of *w* and *b*, using which the predicted y value will be closed to the actual data when training\n",
        "- For a given x input, the output value that the function predicts is called *y-hat*\n",
        "\n",
        "#### Finding the values of *w* and *b*\n",
        "##### Cost Function\n",
        "- A cost function evaluates how well the model's prediction matches wtih the actual data.\n",
        "- It quantifies the error between the predicted value and the actual value\n",
        "- The goal of learning algorithm is to minimize this cost function, thereby improving the accuracy of the model\n",
        "- **Commonly used Cost functions for Regression**\n",
        " - Mean Squared Error : *Measures the average of the squared differences between predicted and actual values*\n",
        " - Mean Absolute Error : *Measures the average of the absolute differences between precited and actual values*\n",
        " - Huber Loss : *Combines the properties of MSE and MAE*\n",
        " - Mean Squared Logarithmic Error\n",
        "\n",
        "**We'll use Mean Squared Error Cost Function here**\n",
        "\n",
        "#### Mean Squared Error Cost Function\n",
        "- Error = *y-hat - y*\n",
        "- Squared error = $(\\hat{y}^i - y^i)^2$\n",
        "- Sum of squared errors for m number of training set = $\\sum_{i=1}^m(\\hat{y}^i - y^i)^2$\n",
        "- Mean of squared error = $\\frac{1}{m}\\sum_{i=1}^m(\\hat{y}^i - y^i)^2$\n",
        "- We'll also divide the entire function by 2, just to reduce the number and look clean, this will not change the results, so this is optional\n",
        "- Final Cost function of MSE $J_{(w,b)}$ = $\\frac{1}{2m}\\sum_{i=1}^m(\\hat{y}^i - y^i)^2$ = $\\frac{1}{2m}\\sum_{i=1}^m(f_{w,b}(x^i) - y^i)^2$\n",
        "\n",
        "**Now we have to find a way to minimize the cost function so that the error is very less and the predictions are as closest as possible**\n",
        "\n",
        "##### Algoritms that can be used to minimize cost functions\n",
        "- Gradient based algorithms\n",
        "  - Gradient Descent\n",
        "    - Batch Gradient Descent\n",
        "    - Stochastic Gradient Descent\n",
        "    - Mini-batch gradient descent\n",
        "  - Variants of Gradient descent\n",
        "    - Momentum\n",
        "    - Nesterov Accelerated Gradient\n",
        "    - Adagrad\n",
        "    - Adadelta\n",
        "    - RMSprop\n",
        "    - Adam\n",
        "- Second-Order Methods\n",
        "  - Newton's Method\n",
        "  - Quasi-Newton Methods\n",
        "    - Broyden-Fletcher-Goldfarb-Shanno\n",
        "    - Limited-memory BFGS\n",
        "- Derivative-Free Optimization\n",
        "  - Genetic Algorithms\n",
        "  - Simulated Annealing\n",
        "  - Particle Swarm Optimization\n",
        "  - Nelder-Mead (Simplex) Method\n",
        "- Convex Optimization\n",
        "  -Interior-Point Methods\n",
        "  - Dual Ascent and Dual Decomposition\n",
        "- Specialized Algorithms\n",
        "  - Expectation-Maximixation\n",
        "  - Support vector Machines\n",
        "-Bayesian Optimization\n",
        "  - Gaussian Processes\n",
        "- Reinforcement Learning\n",
        "  - Policy Gradient Methods\n",
        "\n",
        "\n",
        "##### Minimizing cost function using *Gradient descent* Algorithm\n",
        "- Initialize parameters : Start wtih initial guesses for w,b\n",
        "- Compute gradients :\n",
        "  - Calculate partial derivateives of cost function j with respect to w and b\n",
        "  - $\\frac{\\partial J}{\\partial w}$ = $\\frac{1}{m}\\sum_{i=1}^m(f_{w,b}(x)^i - y^i)x^i$\n",
        "  - $\\frac{\\partial J}{\\partial b}$ = $\\frac{1}{m}\\sum_{i=1}^m(f_{w,b}(x)^i - y^i)$\n",
        "- Update Parameters\n",
        "  - Adjust w,b in the direction of the negative gradient to reduce the cost\n",
        "  - w = w - α$\\frac{\\partial J}{\\partial w}$\n",
        "  - b = b - α$\\frac{\\partial J}{\\partial b}$\n",
        "  - α is the learning rate (a small positive number that controls the step size)\n",
        "- Iterate : repeat the gradient computation and parameter update steps until the cost function converges to a minimum\n",
        "- Convergence : The algorithm converges when the changes in w,b become very small\n",
        "- By iteratively adjusting the parameters *w* and *b* using the gradient descent algorithm, we minimize the MSE and thus improve the accuracy of our linear regression model in predicting the target variable.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IaQsRJdiVnws"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}